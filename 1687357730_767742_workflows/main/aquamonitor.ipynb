{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Aqua Monitor using OpenEO\n",
    "\n",
    "This notebook implements the [Aqua Monitor](https://aqua-monitor.appspot.com/) algorithm using [OpenEO](https://openeo.org/), an open source processing platfrom for earth observation data.\n",
    "\n",
    "The goal of aquamonitor is to provide a planetary-scale view on changes in land and water occurrence over time. In other words, it shows how land has changed to surface water and vice-versa. Applications like this give insight in both man-made interventions, as well as changes due to natural variability and climate change.\n",
    "\n",
    "In this notebook, a global view (although you could put it in the input parameters) is not the goal. For a given area of interest (aoi), the notebook builds a set of instructions using the [OpenEO python client library](https://open-eo.github.io/openeo-python-client/) and sends it to the [OpenEO Platfrom](https://openeo.cloud/) backend for processing. Then the results from the analysis are downloaded and interactively plotted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "This notebook has a few dependencies. The main ones being the OpenEO python client library. We also use xarray to analyse downloaded datacubes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install --upgrade \"shapely<1.8.0\" \"openeo==0.11.0\" geoviews bokeh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import math\n",
    "from datetime import datetime\n",
    "\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from openeo import connect, Connection\n",
    "from openeo.rest.datacube import DataCube\n",
    "from typing import List, Dict, Tuple, Union\n",
    "from pathlib import Path\n",
    "import xarray as xr\n",
    "\n",
    "from utils import get_files_from_dc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation imports\n",
    "import cartopy.crs as ccrs\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import geoviews as gv\n",
    "import holoviews as hv\n",
    "import numpy as np\n",
    "\n",
    "from holoviews import opts, streams\n",
    "from holoviews.element.tiles import OSM\n",
    "\n",
    "gv.extension(\"bokeh\",\"matplotlib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to the backend\n",
    "OpenEO features a generic API that can be implemented using many different backends using different implementations. To use the a certain backend, you need to use a backend url and provide credentials. Here we log in with one of those backends using OIDC, using egi as a provider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to VITO backend\n",
    "vito_url: str = \"https://openeo.vito.be/openeo/1.0\"\n",
    "openeo_platform_url: str = \"openeo.cloud\"\n",
    "\n",
    "backend_url: str = vito_url\n",
    "con: Connection = connect(backend_url)\n",
    "con.authenticate_oidc(provider_id=\"egi\")\n",
    "\n",
    "out_dir = Path(\"output\")\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "local_cache_file: Path = out_dir / \"job_cache.json\"\n",
    "debug = True\n",
    "\n",
    "collection_id = \"TERRASCOPE_S2_TOC_V2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore collections and processes\n",
    "You can interactively explore different collections in the notebook using the `Connection` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# con.list_collections()\n",
    "con.describe_collection(collection_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also list the processes available in the backend use the `list_processes` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# con.list_processes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the available imagery\n",
    "Processing a `DataCube` starts with loading a collection or multiple collections that the backend has available.\n",
    "Here we download just the Sentinel2 imagery, filtering on the spatial and temporal extent that we are interested in. As processing for large areas takes time, we calculate for smaller areas if we are debugging or showing off the script (using `debug = True` ).\n",
    "As the backend that we use loads the data in UTM projection, we calculate the UTM zone number based on the bounding box, so we can plot the results on a map later on.\n",
    "Finally, we also specify `plot_resolution` and `smoothing_resolution` here. `plot_resolution` is used later on. This is mainly relevant when calculating for larger areas, as plotting too many pixels is not useful and very tough on the local hardware. `smoothing_resolution` is explained in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_utm_zone(lon: float) -> int:\n",
    "    return math.ceil((180 + lon) / 6)\n",
    "\n",
    "# Define constraints for loading\n",
    "embalse_de_barbate_bbox: Dict[str, Union[float, str]] = {\n",
    "    \"west\": -5.707740783691406,\n",
    "    \"east\": -5.6764984130859375,\n",
    "    \"south\": 36.40221798067486,\n",
    "    \"north\": 36.4315034892636,\n",
    "    \"crs\": \"EPSG:4326\"\n",
    "}\n",
    "sw_spain_bbox: Dict[str, Union[float, str]] = {\n",
    "    \"west\": -7.682155041704681,\n",
    "    \"east\": -5.083888440142181,\n",
    "    \"south\": 36.18203953636458,\n",
    "    \"north\": 38.620982842287496,\n",
    "    \"crs\": \"EPSG:4326\"\n",
    "}\n",
    "\n",
    "if debug:\n",
    "    spatial_extent: Dict[str, Union[float, str]] = embalse_de_barbate_bbox\n",
    "    utm_zone = get_utm_zone(embalse_de_barbate_bbox[\"west\"])\n",
    "else:\n",
    "    spatial_extent: Dict[str, Union[float, str]] = sw_spain_bbox\n",
    "    utm_zone = get_utm_zone(sw_spain_bbox[\"west\"])\n",
    "\n",
    "smoothing_resolution: float = 30.0  # meters\n",
    "if debug:\n",
    "    plot_resolution: float = 10.0 # meters\n",
    "else:\n",
    "    plot_resolution: float = 1000.0  # meters\n",
    "\n",
    "if debug:\n",
    "    t_end = datetime.utcnow()\n",
    "    t_start = t_end - relativedelta(years=2)\n",
    "    temporal_extent: List[str] = [str(t_start), str(t_end)]\n",
    "else:\n",
    "    temporal_extent: List[str] = [\"2019-01-01\", \"2022-01-01\"]\n",
    "\n",
    "band_names = [\"swir\", \"nir\", \"green\"]\n",
    "bands = [\"B11\", \"B08\", \"B03\"]\n",
    "        \n",
    "dc: DataCube = con.load_collection(\n",
    "    collection_id=collection_id,\n",
    "    spatial_extent=spatial_extent,\n",
    "    temporal_extent=temporal_extent,\n",
    "    bands=bands\n",
    "    ).add_dimension(name=\"source_name\", label=collection_id, type=\"other\") \\\n",
    "    .rename_labels(dimension=\"bands\", source=bands, target=band_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Source Data\n",
    "To explore the source data, it is possible to download this data. There are two ways of doing this. Synchronously with the `.download` method on the `DataCube` object, or asynchronously by starting a batch job. For production or large jobs, it is better to do this using a batch job, as synchronous jobs are tied to a timeout. The processing speed of a synchronous job also depends on the load on the server, while batch jobs are queued up and always executed when there is space on the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synchronous download\n",
    "# dc.download(out_dir / \"raw.nc\", format=\"netcdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asynchronous download\n",
    "\n",
    "from utils import get_files_from_dc\n",
    "files = get_files_from_dc(\n",
    "    dc,\n",
    "    out_directory=out_dir / \"raw\",\n",
    "    job_name=\"get_collection\",\n",
    "    local_cache_file=out_dir / \"job_cache.json\",\n",
    "    recalculate=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set_raw: xr.Dataset = xr.open_dataset(out_dir / \"raw\" / \"openEO.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Raw data\n",
    "\n",
    "Here we use [Open Street Map](https://www.openstreetmap.org) as well as the [holoviews](https://holoviews.org/) package, and its extension, [geoviews](https://geoviews.org/). Other standard tools such as bokeh (plotting backend) and cartopy (crs transforms) are used here as well.\n",
    "In other plots, we use matplotlib functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kdims = [\"x\", \"y\", \"t\"]\n",
    "vdims = [\"swir\", \"nir\", \"green\"]\n",
    "\n",
    "data_set_raw_fixed = data_set_raw.drop(\"crs\")\n",
    "hv.Dimension.type_formatters[np.datetime64] = '%Y-%m-%d'  # readable time format\n",
    "gv_raw = gv.Dataset(data_set_raw_fixed, kdims=kdims, vdims=vdims, crs=ccrs.UTM(utm_zone)).redim(x=\"lon\", y=\"lat\")\n",
    "\n",
    "print(repr(gv_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dmap = gv_raw.to(gv.Image, [\"lon\", \"lat\"], \"green\", group=\"raw_data\", label=\"raw\", datatype=[\"xarray\"], dynamic=True)\n",
    "overlay = OSM() * dmap\n",
    "overlay.opts(\n",
    "    opts.Image(cmap=\"turbo\", colorbar=True, clim=(0, 7000), height=500, width=500, tools=[\"hover\"]),\n",
    "    opts.Tiles(height=500, width=500))\n",
    "\n",
    "overlay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Smoothen images\n",
    "In order for the algorithm to function better, images are slightly smoothed spatially, using the `.resample_spatial` method. Note that the spatial resolution of the Sentinel-2 RGB bands is 10m."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoothed_dc = dc.resample_spatial(method=\"cubic\", resolution=smoothing_resolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divide the data into time-bands\n",
    "Filtering clouds and other atmospheric effects is notoriously difficult, especially if the algorithm has to work globally. Luckily, we are interested on long-term changes, which means that we can make use of composite images that use multiple years of data to achieve a good cloud-free image. Here we assume that over the time period chosen, changes such as seasonal effects do not skew our composite image by a large degree.\n",
    "The smoothed input data is divided in bands of a number of years (`year_interval`). The data is sorted and the n-th percentile of every time-band (`percentile`) is used for further analysis. The percentile is based on the MODIS cloud cover percentage dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from datetime import timedelta\n",
    "import pandas as pd\n",
    "\n",
    "year_interval: int = 1\n",
    "percentile: float = 0.2  # fractions [0 - 1]\n",
    "bucketed_netcdf_filename: str = \"bucketed.nc\"\n",
    "\n",
    "if debug:\n",
    "    dr: pd.DatetimeIndex = pd.date_range(start=temporal_extent[0], end=temporal_extent[1], periods=3)\n",
    "else:\n",
    "    dr: pd.DatetimeIndex = pd.date_range(start=temporal_extent[0], end=temporal_extent[1], freq=f\"{year_interval}YS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openeo import processes\n",
    "\n",
    "if debug:\n",
    "    image_count_limit: int = 5  # Will exclude some images\n",
    "else:\n",
    "    image_count_limit: int = 10\n",
    "\n",
    "t_intervals = [[str(d), str(dr[i+1])] for i, d in enumerate(dr[:-1])]\n",
    "\n",
    "# Create bucketed DC based on percentile of images\n",
    "t_bucketed_dc: DataCube = smoothed_dc \\\n",
    "    .aggregate_temporal(\n",
    "        intervals=t_intervals,\n",
    "        reducer=lambda data: processes.quantiles(data, probabilities=[percentile]),\n",
    "        labels=[t_int[0] for t_int in t_intervals]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a cache in the OpenEO backend\n",
    "Because we are severely cutting down on the size of the total dataset in the previous timebanding step, caching the data here helps quick development and saves many cloud resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_or_create_cached_cube\n",
    "if not debug:\n",
    "    t_bucketed_dc = get_or_create_cached_cube(\n",
    "        dc=t_bucketed_dc,\n",
    "        local_cache_file=local_cache_file,\n",
    "        temporal_extent=tuple(temporal_extent),\n",
    "        spatial_extent={\n",
    "            \"x\": (spatial_extent[\"west\"], spatial_extent[\"east\"]),\n",
    "            \"y\": (spatial_extent[\"north\"], spatial_extent[\"south\"])\n",
    "        },\n",
    "        job_name=\"t_bucketing_aquamonitor\",\n",
    "        result_format=\"gtiff\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Count the number of images included in each interval\n",
    "To check whether there are enough images available for a meaningful composite image, we filter for a certain number of images present in each timestep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Workaround for now is to set images to 1 until count works\n",
    "count_dc: DataCube = smoothed_dc.band(\"green\") \\\n",
    "    .apply(lambda x: x * 0 + 1) \\\n",
    "    .aggregate_temporal(\n",
    "        intervals=t_intervals,\n",
    "        reducer=lambda data: processes.sum(data),\n",
    "        labels=[t_int[0] for t_int in t_intervals]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the time-bucketed NDWI cube\n",
    "To detect surface-to-water changes and vice-versa, we rely on the NDWI: $$NDWI = \\frac{\\rho_{green} - \\rho_{nir}}{\\rho_{green} + \\rho_{nir}}$$\n",
    "and on the MNDWI: $$MNDWI = \\frac{\\rho_{green} - \\rho_{swir}}{\\rho_{green} + \\rho_{swir}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create products\n",
    "from typing import Tuple\n",
    "\n",
    "green: DataCube = t_bucketed_dc.band(\"green\")\n",
    "nir: DataCube = t_bucketed_dc.band(\"nir\")\n",
    "swir: DataCube = t_bucketed_dc.band(\"swir\")\n",
    "ndwi: DataCube = (green - nir) / (green + nir)\n",
    "mndwi: DataCube = (green - swir) / (green + swir)\n",
    "\n",
    "# Rescale\n",
    "def rescale(dc: DataCube, factors: Tuple[float]) -> DataCube:\n",
    "    return dc.subtract(factors[0]).divide(factors[1] - factors[0])\n",
    "\n",
    "rescaled_mndwi = rescale(mndwi, factors=(-0.6, 0.6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the MNDWI results\n",
    "For the interactive plot later in the notebook, we need to get the MNDWI results in as a Cloud Optimized GeoTiff. We can accomplish this by saving to the geotiff format and using a batch format. For now, the results of a batch operation stay reachable in the openEO backend for 3 months.\n",
    "Using the Cloud Optimized Geotiff, we can query coordinates in the GeoTiff without downloading it, or opening it fully. This is further explained at: https://www.cogeo.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_urls_from_dc\n",
    "mndwi_urls = get_urls_from_dc(\n",
    "    rescaled_mndwi.resample_spatial(method=\"cubic\", resolution=plot_resolution),\n",
    "    'mndwi_calculation',\n",
    "    result_format=\"gtiff\",\n",
    "    local_cache_file=local_cache_file,\n",
    "    recalculate=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mndwi_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the mndwi results\n",
    "Here we use the same plotting libraries as before to visually explore the MNDWI dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_dataset(mndwi_urls[0], engine=\"rasterio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = xr.Variable(dims=\"t\", data=dr[:-1])\n",
    "data_set_mndwi = xr.concat([xr.open_dataset(url, engine=\"rasterio\") for url in mndwi_urls], dim=dim)\n",
    "da_mndwi = data_set_mndwi.rename(band_data=\"mndwi\").drop(\"band\")[\"mndwi\"]\n",
    "da_mndwi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kdims = [\"x\", \"y\", \"t\"]\n",
    "vdims = [\"mndwi\"]\n",
    "\n",
    "hv.Dimension.type_formatters[np.datetime64] = '%Y-%m-%d'  # readable time format\n",
    "lon = hv.Dimension(\"lon\", label=\"longitude\", unit=\"deg Northing\")\n",
    "lat = hv.Dimension(\"lat\", label=\"latitude\", unit=\"deg Easting\")\n",
    "gv_mndwi = gv.Dataset(da_mndwi, kdims=kdims, vdims=vdims, crs=ccrs.UTM(utm_zone)).redim(x=lon, y=lat)\n",
    "\n",
    "img = gv_mndwi.to(gv.Image, [\"lon\", \"lat\"], \"mndwi\", group=\"mndwi\", label=\"mndwi\", datatype=[\"xarray\"], dynamic=True)\n",
    "img = img.redim(x=lon, y=lat)\n",
    "overlay = OSM() * img\n",
    "overlay.opts(\n",
    "    opts.Image(cmap=\"turbo\", colorbar=True, clim=(-1.2, 1.2), height=500, width=500, alpha=0.8),\n",
    "    opts.Tiles(height=500, width=500))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform a linear regression on each band\n",
    "The aqua monitor algorithm uses the ndwi as the wetness indicator. \n",
    "The linear regression for the ndwi is used used to detect changes. The slope of the linear regression will be the indicator water-to-surface changes or vice-versa.\n",
    "As a linear regression is defined as a process in the backend yet, OpenEO allows us to execute python code on the DataCube using a UDF (User Defined Function).\n",
    "Here we load the UDF from this repository and apply it to the DataCube using `reduce_temporal_udf`, as we will lose the `t` dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_udf(udf_path: Union[str, Path]):\n",
    "    with open(udf_path, \"r+\") as f:\n",
    "        return f.read()\n",
    "\n",
    "linear_regression_udf_path: Path = Path(Path.cwd().parent / \"udfs\" / \"linear_regression.py\")\n",
    "linear_regression_udf: str = load_udf(linear_regression_udf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg: DataCube = rescaled_mndwi.reduce_temporal_udf(code=linear_regression_udf, runtime=\"Python\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Try out the linear regression locally\n",
    "It is possible to try out UDFs locally. Therefore we need to download the input data locally and run the udf using `execute_local_udf`. This is useful when debugging your UDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_files_from_dc\n",
    "\n",
    "mndwi_dir = out_dir / \"mndwi_cache\"\n",
    "\n",
    "mndwi_files = get_files_from_dc(  # reuse the job for the mndwi urls\n",
    "    rescaled_mndwi.resample_spatial(method=\"cubic\", resolution=plot_resolution),\n",
    "    mndwi_dir,\n",
    "    'mndwi_calculation',\n",
    "    result_format=\"gtiff\",\n",
    "    local_cache_file=local_cache_file,\n",
    "    recalculate=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go from geotiffs to a xarray dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "import re\n",
    "\n",
    "import rioxarray\n",
    "\n",
    "mndwi_paths = list(mndwi_dir.glob(\"*.tif\"))\n",
    "mndwi_pd = [\n",
    "    (\n",
    "        date(\n",
    "            *map(lambda g: int(g), re.match(r\".+(\\d{4})-(\\d{2})-(\\d{2}).+\", path.name).groups())\n",
    "        ),\n",
    "        path\n",
    "    ) for path in mndwi_paths\n",
    "]\n",
    "\n",
    "t = xr.Variable(dims=\"t\", data=dr[:-1])\n",
    "das = []\n",
    "for d, p in mndwi_pd:\n",
    "    da = rioxarray.open_rasterio(p)\n",
    "    coords = da.coords\n",
    "    coords.update({\"t\": d})\n",
    "    da = da.assign_coords(coords)\n",
    "    das.append(da)\n",
    "combined: xr.DataArray = xr.concat(das, dim=t)\n",
    "\n",
    "ds: xr.Dataset = combined.to_dataset('band').rename({1: \"var\"})\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save dataset as a file for use in the udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netcdf_file = Path(out_dir / \"mndwi_cache.nc\")\n",
    "netcdf_file.unlink(True)\n",
    "ds.to_netcdf(out_dir / \"mndwi_cache.nc\", mode=\"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openeo.udf import execute_local_udf\n",
    "from openeo.udf.udf_data import UdfData\n",
    "result: UdfData = execute_local_udf(linear_regression_udf, out_dir / \"mndwi_cache.nc\", fmt='netcdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtain the results using the `datacube_list` property on the result object `UdfData`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_da = result.datacube_list[0].get_array()\n",
    "result_da"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the result of the local udf execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(result_da.sel(bands=\"var\"))\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "significant changes are based on a threshold, as calculated in the next step. For now, let's see if we have significant changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(abs(result_da.sel(bands=\"var\")) > 0.12497148722627738)  # this number comes from the next step\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Masking\n",
    "In order to create an easier to read result image, a couple of masks are combined to create the resulting image:\n",
    "Masks are created for areas that:\n",
    "- have too few images\n",
    "- have no clear changes from land-to-water or vice versa\n",
    "- areas which maximum mndwi value is still clearly land\n",
    "- areas which minimum mndwi value is still clearly water\n",
    "pixels with value 1 are values that should be left out of the end result.\n",
    "\n",
    "To combine masks, we invert the masks so that pixels with value 0 should be left out, and use the product of the masks to create an overlay. We then invert the result again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openeo import processes\n",
    "from dateutil.parser import parse\n",
    "from datetime import datetime\n",
    "\n",
    "ndwi_min_water: float = -0.05 # minimum value of NDWI to assume as water\n",
    "ndwi_max_land: float = 0.5\n",
    "slope_threshold: float = 0.025\n",
    "start: datetime = parse(temporal_extent[0])\n",
    "stop: datetime = parse(temporal_extent[1])\n",
    "\n",
    "if debug:\n",
    "    min_num_images: int = 5\n",
    "else:\n",
    "    min_num_images: int = 20\n",
    "\n",
    "# 15 / (stop_year - start_year)\n",
    "slope_threshold_ratio: float = 15 * 365.25 / float(stop.__sub__(start).days)\n",
    "print(f\"slope threshold: {slope_threshold * slope_threshold_ratio}\")\n",
    "\n",
    "mndwi_min: DataCube = mndwi.reduce_dimension(dimension=\"t\", reducer=\"min\")\n",
    "mndwi_max: DataCube = mndwi.reduce_dimension(dimension=\"t\", reducer=\"max\")\n",
    "# Should always be water\n",
    "min_water_mask: DataCube = mndwi_min.apply(lambda val: processes.lt(x=val, y=ndwi_max_land))  # invert mask, should be processes.gte\n",
    "# should always be land\n",
    "max_land_mask: DataCube = mndwi_max.apply(lambda val: processes.gt(x=val, y=ndwi_min_water))  # invert mask, should be processes.lte\n",
    "\n",
    "# Create bucketed DC based on image count\n",
    "num_images_mask: DataCube = smoothed_dc.band(\"green\") \\\n",
    "    .apply(lambda data: processes.add(x=processes.multiply(x=data, y=0), y=1)) \\\n",
    "    .aggregate_temporal(\n",
    "        intervals=t_intervals,\n",
    "        reducer=lambda data: processes.sum(data),\n",
    "        labels=[t_int[0] for t_int in t_intervals]\n",
    "    ) \\\n",
    "    .apply(lambda val: processes.gte(x=val, y=image_count_limit)) \\\n",
    "    .apply(lambda val: processes.multiply(x=val, y=1.)) \\\n",
    "    .reduce_dimension(\n",
    "        dimension=\"t\",\n",
    "        reducer=lambda data: processes.product(data)\n",
    "    )\n",
    "\n",
    "lin_reg_mask: DataCube = lin_reg.apply(processes.absolute).apply(lambda val: processes.gte(x=val, y=slope_threshold * slope_threshold_ratio))\n",
    "\n",
    "mask: DataCube = lin_reg_mask.multiply(1.) \\\n",
    "    .multiply(min_water_mask.multiply(1.)) \\\n",
    "    .multiply(max_land_mask.multiply(1.)) \\\n",
    "    .multiply(num_images_mask.multiply(1.)) \\\n",
    "    .apply(lambda data: processes.eq(x=data, y=0)) \\\n",
    "    .multiply(1.)  # after multiplying masks, invert\n",
    "\n",
    "lin_reg = lin_reg.mask(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the Linear Regression results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg_files = get_files_from_dc(\n",
    "    lin_reg,\n",
    "    out_directory=out_dir / \"lin_reg\",\n",
    "    job_name=\"lin_reg\",\n",
    "    local_cache_file=out_dir / \"job_cache.json\",\n",
    "    result_format=\"GTiff\",\n",
    "    recalculate=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or download files from a recent successful job:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the results of the Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set_lin_reg: xr.Dataset = xr.open_dataset(out_dir / \"lin_reg\" / \"openEO.tif\").rename(band_data=\"lin_reg\")\n",
    "data_set_lin_reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we can take more time and plo the results on a map projection with OpenStreetMap in the background. We also want to have some interactivity by checking where the linear regression data comes from. We use the source data from the rescaled MNDWI to create an interactive timeseries when a pixel is selected on the Map. We do not want to download the full dataset locally, so we choose to use the COG (cloud optimized geotiffs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cartopy.crs as ccrs\n",
    "\n",
    "import geoviews as gv\n",
    "import holoviews as hv\n",
    "import numpy as np\n",
    "import panel as pn\n",
    "\n",
    "from holoviews import opts, streams\n",
    "from holoviews.element.tiles import OSM\n",
    "\n",
    "gv.extension(\"bokeh\",\"matplotlib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Clip the image in case of a large area:\n",
    "If taking the entirety of SW spain, we still have a problem plotting an image this large in jupyterlab, so we take an interesting area:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not debug:\n",
    "    from rasterio.crs import CRS\n",
    "    from rasterio.windows import from_bounds\n",
    "    from pyproj import Transformer\n",
    "\n",
    "    bounds: Dict[str, float] = {\n",
    "        \"top\": -6.5,\n",
    "        \"right\": 36.5,\n",
    "        \"bottom\": -5.5,\n",
    "        \"left\": 37.5\n",
    "    }\n",
    "\n",
    "    crs_source = CRS.from_string(\"epsg:4326\")\n",
    "    crs_dest = rasterio.open(urls[0]).crs\n",
    "\n",
    "    transformer = Transformer.from_crs(crs_source, crs_dest)\n",
    "    bounds[\"left\"], bounds[\"top\"] = transformer.transform(bounds[\"left\"], bounds[\"top\"])\n",
    "    bounds[\"right\"], bounds[\"bottom\"] = transformer.transform(bounds[\"right\"], bounds[\"bottom\"])\n",
    "\n",
    "    window = from_bounds(**bounds, transform=rasterio.open(urls[0]).transform)\n",
    "\n",
    "    data_set_lin_reg.rio.write_crs(crs_dest, inplace=True)\n",
    "    data_set_lin_reg_clipped: xr.Dataset = data_set_lin_reg.rio.clip_box(maxx=bounds[\"right\"], minx=bounds[\"left\"], maxy=bounds[\"top\"], miny=bounds[\"bottom\"])\n",
    "    data_set_lin_reg_clipped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dynamic plot using the Cloud Optimized Geotiffs stored in the backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kdims = [\"x\", \"y\"]\n",
    "vdims = [\"lin_reg\"]\n",
    "\n",
    "hv.Dimension.type_formatters[np.datetime64] = '%Y-%m-%d'  # readable time format\n",
    "lon = hv.Dimension(\"lon\", label=\"longitude\", unit=\"deg Northing\")\n",
    "lat = hv.Dimension(\"lat\", label=\"latitude\", unit=\"deg Easting\")\n",
    "gv_lin_reg = gv.Dataset(data_set_lin_reg, kdims=kdims, vdims=vdims, crs=ccrs.UTM(zone=utm_zone, southern_hemisphere=False)).redim(x=lon, y=lat)\n",
    "\n",
    "print(repr(gv_lin_reg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rasterio.transform import rowcol\n",
    "from rasterio.windows import Window\n",
    "import rasterio\n",
    "\n",
    "img = gv_lin_reg.to(gv.Image, [\"lon\", \"lat\"], \"lin_reg\", group=\"lin_reg\", label=\"linear regression\", datatype=[\"xarray\"])\n",
    "img = img.redim(x=lon, y=lat)\n",
    "overlay = OSM() * img\n",
    "\n",
    "def get_mndwi_ts(x, y):\n",
    "    \"\"\"\n",
    "    reads from mndwi COG based on coordinates and returns timeseries values\n",
    "    \"\"\"\n",
    "    time = hv.Dimension(\"t\", label=\"time\")\n",
    "    mndwi = hv.Dimension(\"norm mndwi\", label=\"norm MNDWI\")\n",
    "    ts = []\n",
    "    for url in mndwi_urls:\n",
    "        with rasterio.open(url) as src:\n",
    "            xs, ys = rowcol(src.transform, x, y)\n",
    "            with open(out_dir / \"log.txt\", \"w+\") as f:\n",
    "                f.write(f\"\"\"\n",
    "                    x={x}, y={y}\n",
    "                    xs={xs}, ys={ys}\n",
    "                \"\"\")\n",
    "            ts.append(src.read(window=Window(ys, xs, 1, 1)).flatten()[0])\n",
    "    return hv.Curve(ts, time, mndwi)\n",
    "    # return hv.Curve(da_mndwi.sel(x=x, y=y, method=\"nearest\"), time, mndwi)\n",
    "    \n",
    "clicker = streams.Tap(name=\"location\", source=img, x=data_set_lin_reg[\"x\"].values[0], y=data_set_lin_reg[\"y\"].values[0])\n",
    "timeseries = hv.DynamicMap(callback=get_mndwi_ts, streams=[clicker])\n",
    "\n",
    "layout = overlay + timeseries\n",
    "layout.opts(\n",
    "    opts.Image(cmap=\"turbo\", colorbar=True, clim=(-1, 1), height=500, width=500, tools=[\"hover\"], alpha=0.8),\n",
    "    opts.Tiles(height=500, width=500),\n",
    "    opts.Curve(height=200, width=300, ylim=(0, 2)))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "308f0e6c08d7a2e18fdd0dc9d195c5dd386da1d83ed1220cd08f0a2e89ee0788"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
